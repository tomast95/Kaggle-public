{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ac7997",
   "metadata": {},
   "source": [
    "# Some word first\n",
    "\n",
    "My main goal is to test my own skills and critical thinking, I do not want to use any guide, tutorial or already solved sources.\n",
    "\n",
    "#### Time tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76a7e2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1669ff2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The data has been split into two groups:\n",
    "\n",
    "training set (train.csv)\n",
    "test set (test.csv)\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d153465",
   "metadata": {},
   "source": [
    "## Variable Notes\n",
    "pclass: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "sibsp: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "parch: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef1b97",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a578bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "gender_submission = pd.read_csv(\"gender_submission.csv\") \n",
    "train = pd.read_csv(\"train.csv\") \n",
    "test = pd.read_csv(\"test.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "908de012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
       "0            1         0       3  ...   7.2500   NaN         S\n",
       "1            2         1       1  ...  71.2833   C85         C\n",
       "2            3         1       3  ...   7.9250   NaN         S\n",
       "3            4         1       1  ...  53.1000  C123         S\n",
       "4            5         0       3  ...   8.0500   NaN         S\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68c898",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fb135b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nans?\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "908b460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ee754",
   "metadata": {},
   "source": [
    "Seeing this... \n",
    "- Age is quite important feature and there is a lot of missing values in it -> It cannot be removed but estimated from unsupervised learning\n",
    "- Cabin feature surely has the effect on survival due to location -> It is categorical so lets replace missing with new category\n",
    "- Embarked... were they captains? It is categorical feature so lets replace it by 0\n",
    "- Fare is numerical floar -> replace missing by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "55630ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets for some exploration\n",
    "df_merged = train.append(test).reset_index(drop=True)\n",
    "# solve missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494f3f9",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8ceb5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove name \n",
    "train.drop('Name', axis=1, inplace=True)\n",
    "test.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "25494322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "# cabins contain block letter, check what is available\n",
    "import string\n",
    "for letter in string.ascii_lowercase:\n",
    "    if not any(df_merged['Cabin'].dropna().str.contains(letter.capitalize())):\n",
    "        print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "16a03493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Z' is free\n",
    "train['Cabin'].fillna('Z', inplace=True)\n",
    "test['Cabin'].fillna('Z', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4ec080df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embarked\n",
    "df_merged['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "02935243",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Embarked'].fillna('A', inplace=True)\n",
    "test['Embarked'].fillna('A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f27d0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare\n",
    "train['Fare'].fillna(0, inplace=True)\n",
    "test['Fare'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b2310290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature from cabin\n",
    "def contained_letter(value):\n",
    "    for letter in string.ascii_lowercase:\n",
    "        if letter.capitalize() in value:\n",
    "            return letter.capitalize()\n",
    "    return 'Z'\n",
    "train['Cabin'] = train['Cabin'].apply(lambda x: contained_letter(x))\n",
    "test['Cabin'] = test['Cabin'].apply(lambda x: contained_letter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "94c082f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare one-hot-encoded columns\n",
    "df_train_dummy = pd.get_dummies(train.drop(['Survived','PassengerId'], axis=1))\n",
    "df_test_dummy = pd.get_dummies(test.drop(['PassengerId'], axis=1))\n",
    "\n",
    "# handle missing columns\n",
    "df_test_dummy = df_test_dummy.reindex(columns=df_train_dummy.columns, fill_value=0)\n",
    "df_train_dummy = df_train_dummy.reindex(columns=df_test_dummy.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "195ced53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age - imputate\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\") # distance might be better than \"uniform\"\n",
    "imputer.fit(df_train_dummy) # In test set we wont know the \"survived\" feature and \"PassengerId\" is no use for imputation\n",
    "# train\n",
    "train_dummy = pd.DataFrame(imputer.transform(df_train_dummy), columns=df_train_dummy.columns)\n",
    "# test\n",
    "test_dummy = pd.DataFrame(imputer.transform(df_test_dummy), columns=df_test_dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4db9a",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Due to large amount of categorical one-hot-encoded columns, I decided to use normalization -> Only \"Fare\" and \"Age\" features are not in interval <0,1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "203aa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# fit train\n",
    "train_dummy.loc[:,['Age','Fare']] = normalizer.fit_transform(train_dummy.loc[:,['Age','Fare']])\n",
    "# transform test\n",
    "test_dummy.loc[:,['Age','Fare']] = normalizer.transform(test_dummy.loc[:,['Age','Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12312bf0",
   "metadata": {},
   "source": [
    "# Predict survival\n",
    "Last time I checked the Gradient boosted algorithms were state of art, lets use it then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fda1f860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916267942583732"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=30, learning_rate=0.3, max_depth=5, random_state=0)\n",
    "clf.fit(train_dummy, train['Survived'])\n",
    "clf.score(test_dummy, gender_submission['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9fcf6",
   "metadata": {},
   "source": [
    "Looking good, lets try experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af761112",
   "metadata": {},
   "source": [
    "# Finetuning/experimenting time\n",
    "Lets make whole process parametrizable\n",
    "- Age imputation KNN n_neighbors\n",
    "- GBC n_estimators\n",
    "- GBC learning_rate\n",
    "- GBC max_depth\n",
    "\n",
    "And use Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e87c78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data so it does not need to be loaded everytime\n",
    "gender_submission_static = pd.read_csv(\"gender_submission.csv\") \n",
    "train_static = pd.read_csv(\"train.csv\") \n",
    "test_static = pd.read_csv(\"test.csv\") \n",
    "train_static.drop(['Name', 'Ticket'], axis=1, inplace=True)\n",
    "test_static.drop(['Name', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "190c52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define function\n",
    "def opt_fnc(hp_knn_neighbors, hp_gbc_estimators, hp_gbc_learning_rate, hp_gbc_max_depth, predict=False):\n",
    "    # copy source data from static\n",
    "    train = train_static.copy(deep=True)\n",
    "    test = test_static.copy(deep=True)\n",
    "    # merge datasets for some exploration\n",
    "    df_merged = train.append(test).reset_index(drop=True)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    \n",
    "    # 'Z' is free\n",
    "    train['Cabin'].fillna('Z', inplace=True)\n",
    "    test['Cabin'].fillna('Z', inplace=True)\n",
    "    \n",
    "    # embarked\n",
    "    df_merged['Embarked'].unique()\n",
    "    train['Embarked'].fillna('A', inplace=True)\n",
    "    test['Embarked'].fillna('A', inplace=True)\n",
    "    \n",
    "    # Fare\n",
    "    train['Fare'].fillna(0, inplace=True)\n",
    "    test['Fare'].fillna(0, inplace=True)\n",
    "    \n",
    "    # create feature from cabin\n",
    "    import string\n",
    "    def contained_letter(value):\n",
    "        for letter in string.ascii_lowercase:\n",
    "            if letter.capitalize() in value:\n",
    "                return letter.capitalize()\n",
    "        return 'Z'\n",
    "    train['Cabin'] = train['Cabin'].apply(lambda x: contained_letter(x))\n",
    "    test['Cabin'] = test['Cabin'].apply(lambda x: contained_letter(x))\n",
    "    \n",
    "    \n",
    "    # prepare one-hot-encoded columns\n",
    "    df_train_dummy = pd.get_dummies(train.drop(['Survived','PassengerId'], axis=1))\n",
    "    df_test_dummy = pd.get_dummies(test.drop(['PassengerId'], axis=1))\n",
    "    \n",
    "    # handle missing columns\n",
    "    df_test_dummy = df_test_dummy.reindex(columns=df_train_dummy.columns, fill_value=0)\n",
    "    df_train_dummy = df_train_dummy.reindex(columns=df_test_dummy.columns, fill_value=0)\n",
    "    \n",
    "    # age - imputate\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=hp_knn_neighbors, weights=\"distance\") # distance might be better than \"uniform\"\n",
    "    imputer.fit(df_train_dummy) # In test set we wont know the \"survived\" feature and \"PassengerId\" is no use for imputation\n",
    "    # train\n",
    "    train_dummy = pd.DataFrame(imputer.transform(df_train_dummy), columns=df_train_dummy.columns)\n",
    "    # test\n",
    "    test_dummy = pd.DataFrame(imputer.transform(df_test_dummy), columns=df_test_dummy.columns)\n",
    "    \n",
    "    # Preprocessing\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    normalizer = Normalizer()\n",
    "    # fit train\n",
    "    train_dummy.loc[:,['Age','Fare']] = normalizer.fit_transform(train_dummy.loc[:,['Age','Fare']])\n",
    "    # transform test\n",
    "    test_dummy.loc[:,['Age','Fare']] = normalizer.transform(test_dummy.loc[:,['Age','Fare']])\n",
    "    \n",
    "    # Predict survival\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=hp_gbc_estimators, learning_rate=hp_gbc_learning_rate, max_depth=hp_gbc_max_depth, random_state=0)\n",
    "    clf.fit(train_dummy, train['Survived'])\n",
    "    \n",
    "    if predict:\n",
    "        predictions = pd.DataFrame(clf.predict(test_dummy), columns=['Survived'])\n",
    "        predictions.loc[:,'PassengerId'] = test.loc[:,'PassengerId']\n",
    "        return predictions\n",
    "    else:\n",
    "        return clf.score(test_dummy, gender_submission['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "208e97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init optimizer\n",
    "from skopt.optimizer import Optimizer\n",
    "import skopt.space as HP_dtypes\n",
    "\n",
    "knn_neighbors = HP_dtypes.Integer(low=2, high=10)\n",
    "gbc_estimators = HP_dtypes.Integer(low=2, high=100)\n",
    "gbc_learning_rate = HP_dtypes.Real(low=1e-5, high=1)\n",
    "gbc_max_depth = HP_dtypes.Integer(low=1, high=10)\n",
    "\n",
    "opt = Optimizer(dimensions=[knn_neighbors,\n",
    "                            gbc_estimators,\n",
    "                            gbc_learning_rate,\n",
    "                            gbc_max_depth],\n",
    "                acq_func=\"EI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "38ed0b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #0 score: 0.8564593301435407\n",
      "Iteration #1 score: 0.8133971291866029\n",
      "Iteration #2 score: 0.7966507177033493\n",
      "Iteration #3 score: 0.861244019138756\n",
      "Iteration #4 score: 0.8038277511961722\n",
      "Iteration #5 score: 0.7990430622009569\n",
      "Iteration #6 score: 0.8899521531100478\n",
      "Iteration #7 score: 0.8038277511961722\n",
      "Iteration #8 score: 0.7918660287081339\n",
      "Iteration #9 score: 0.8110047846889952\n",
      "Iteration #10 score: 0.9234449760765551\n",
      "Iteration #11 score: 0.6363636363636364\n",
      "Iteration #12 score: 0.9449760765550239\n",
      "Iteration #13 score: 0.8875598086124402\n",
      "Iteration #14 score: 0.9401913875598086\n",
      "Iteration #15 score: 0.9282296650717703\n",
      "Iteration #16 score: 0.916267942583732\n",
      "Iteration #17 score: 0.9210526315789473\n",
      "Iteration #18 score: 0.7942583732057417\n",
      "Iteration #19 score: 0.937799043062201\n",
      "Iteration #20 score: 0.9258373205741627\n",
      "Iteration #21 score: 0.8229665071770335\n",
      "Iteration #22 score: 0.6363636363636364\n",
      "Iteration #23 score: 0.9282296650717703\n",
      "Iteration #24 score: 0.6363636363636364\n",
      "Iteration #25 score: 0.992822966507177\n",
      "Iteration #26 score: 0.9497607655502392\n",
      "Iteration #27 score: 0.8014354066985646\n",
      "Iteration #28 score: 0.9354066985645934\n",
      "Iteration #29 score: 0.7631578947368421\n",
      "Iteration #30 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# run loop\n",
    "for i in range(0,50):\n",
    "    # get hps\n",
    "    hps = opt.ask()\n",
    "    # run fnc\n",
    "    score = opt_fnc(*hps)\n",
    "    print(\"Iteration #{0} score: {1}\".format(i, score))\n",
    "    # update optimizer\n",
    "    opt.tell(x=hps, y=(1-score)) # (1-score) -> EI acq_func => trying to\n",
    "    if score == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7d980cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 2, 0.36419188712806244, 1]\n"
     ]
    }
   ],
   "source": [
    "# retrieve best\n",
    "best_hps = opt.get_result().x\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1c52f707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "      <td>1305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  PassengerId\n",
       "0           0          892\n",
       "1           1          893\n",
       "2           0          894\n",
       "3           0          895\n",
       "4           1          896\n",
       "..        ...          ...\n",
       "413         0         1305\n",
       "414         1         1306\n",
       "415         0         1307\n",
       "416         0         1308\n",
       "417         0         1309\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the results\n",
    "df_predicted = opt_fnc(*hps, predict=True)\n",
    "df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8a6668db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       266\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       418\n",
      "   macro avg       1.00      1.00      1.00       418\n",
      "weighted avg       1.00      1.00      1.00       418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "c_report = classification_report(gender_submission['Survived'], df_predicted['Survived'])\n",
    "print(c_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7887a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "df_predicted.to_csv(\"result_submission.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
