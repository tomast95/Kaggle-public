{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b55602",
   "metadata": {},
   "source": [
    "# Some word first\n",
    "\n",
    "My main goal is to test my own skills and critical thinking, I do not want to use any guide, tutorial or already solved sources.\n",
    "\n",
    "Knowledge I use is more than 1year old, so there are probably already a lot of new tools\n",
    "\n",
    "#### Time tracking\n",
    "- 14.12.2021 10:00 - 13:00\n",
    "- 14.12.2021 13:20 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9cfe96",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174a2b6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The data has been split into two groups:\n",
    "\n",
    "training set (train.csv)\n",
    "test set (test.csv)\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50765167",
   "metadata": {},
   "source": [
    "## Variable Notes\n",
    "pclass: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "sibsp: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "parch: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc4fd4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d2d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "gender_submission = pd.read_csv(\"gender_submission.csv\") \n",
    "train = pd.read_csv(\"train.csv\") \n",
    "test = pd.read_csv(\"test.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a224cde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
       "0            1         0       3  ...   7.2500   NaN         S\n",
       "1            2         1       1  ...  71.2833   C85         C\n",
       "2            3         1       3  ...   7.9250   NaN         S\n",
       "3            4         1       1  ...  53.1000  C123         S\n",
       "4            5         0       3  ...   8.0500   NaN         S\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672f6c1",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d986115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nans?\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a357fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0a159",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7197631",
   "metadata": {},
   "source": [
    "**Seeing missing values...**\n",
    "- **Age** feature is very important feature and there is a lot of missing values in it -> It cannot be removed so lets try to estimate it by unsupervised learning\n",
    "- **Cabin** feature surely has the effect on survival due to location on ship. We cannot work with the number of cabin since it would create too many features. It would be better to just use the letter of it -> It is categorical so lets replace missing with new category (i.e. deck)\n",
    "- **Embarked**... were missing values they captains? Who knows... It is categorical feature so lets replace it by 0\n",
    "- **Fare** is numerical float -> replace missing by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8176df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove name and Ticket\n",
    "train.drop(['Name', 'Ticket'], axis=1, inplace=True)\n",
    "test.drop(['Name', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b4f7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets for some exploration\n",
    "df_merged = train.append(test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042448cb",
   "metadata": {},
   "source": [
    "### Solve missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "653a297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "# cabins contain block letter, check what is available\n",
    "import string\n",
    "for letter in string.ascii_lowercase:\n",
    "    if not any(df_merged['Cabin'].dropna().str.contains(letter.capitalize())):\n",
    "        print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ca35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Z' is free\n",
    "train['Cabin'].fillna('Z', inplace=True)\n",
    "test['Cabin'].fillna('Z', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b570445f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embarked\n",
    "df_merged['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41304c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Embarked'].fillna('A', inplace=True)\n",
    "test['Embarked'].fillna('A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84499548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare\n",
    "train['Fare'].fillna(0, inplace=True)\n",
    "test['Fare'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d634e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature from cabin\n",
    "def contained_letter(value):\n",
    "    for letter in string.ascii_lowercase:\n",
    "        if letter.capitalize() in value:\n",
    "            return letter.capitalize()\n",
    "    return 'Z'\n",
    "train['Cabin'] = train['Cabin'].apply(lambda x: contained_letter(x))\n",
    "test['Cabin'] = test['Cabin'].apply(lambda x: contained_letter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "950019d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare one-hot-encoded columns\n",
    "df_train_dummy = pd.get_dummies(train.drop(['Survived','PassengerId'], axis=1))\n",
    "df_test_dummy = pd.get_dummies(test.drop(['PassengerId'], axis=1))\n",
    "\n",
    "# handle missing columns\n",
    "df_test_dummy = df_test_dummy.reindex(columns=df_train_dummy.columns, fill_value=0)\n",
    "df_train_dummy = df_train_dummy.reindex(columns=df_test_dummy.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c64d84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age - imputate\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\") # distance might be better than \"uniform\"\n",
    "imputer.fit(df_train_dummy) # In test set we won't know the \"survived\" feature and \"PassengerId\" is no use for imputation neither for predictions\n",
    "# train\n",
    "train_dummy = pd.DataFrame(imputer.transform(df_train_dummy), columns=df_train_dummy.columns)\n",
    "# test\n",
    "test_dummy = pd.DataFrame(imputer.transform(df_test_dummy), columns=df_test_dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d38741",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Due to large amount of one-hot-encoded columns, I decided to use normalization -> Only \"Fare\" and \"Age\" features are not in interval <0,1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ec9d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "# fit & transform train\n",
    "train_dummy.loc[:,['Age','Fare']] = normalizer.fit_transform(train_dummy.loc[:,['Age','Fare']])\n",
    "# transform test\n",
    "test_dummy.loc[:,['Age','Fare']] = normalizer.transform(test_dummy.loc[:,['Age','Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16104f9",
   "metadata": {},
   "source": [
    "# Predict survival\n",
    "Last time I checked the Gradient boosted algorithms were state of art, lets use it then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5214e600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8277511961722488"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=30, learning_rate=0.3, max_depth=5, random_state=0)\n",
    "clf.fit(train_dummy, train['Survived'])\n",
    "clf.score(test_dummy, gender_submission['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709e17a",
   "metadata": {},
   "source": [
    "Looking good, lets try experimenting. It hase randomized nature so the result is not always the same when creating new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e099b",
   "metadata": {},
   "source": [
    "# Finetuning/experimenting time\n",
    "Lets make whole process parametrizable and then use Bayesian optimization\n",
    "- Age imputation KNN n_neighbors\n",
    "- GBC n_estimators\n",
    "- GBC learning_rate\n",
    "- GBC max_depth\n",
    "\n",
    "Baysian Optimization in combination with Gradien Boosted algorithm is probably an overkill for such task but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea8a8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data so it does not need to be loaded everytime\n",
    "gender_submission_static = pd.read_csv(\"gender_submission.csv\") \n",
    "train_static = pd.read_csv(\"train.csv\") \n",
    "test_static = pd.read_csv(\"test.csv\") \n",
    "train_static.drop(['Name', 'Ticket'], axis=1, inplace=True)\n",
    "test_static.drop(['Name', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "105fa5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def opt_fnc(hp_knn_neighbors, hp_gbc_estimators, hp_gbc_learning_rate, hp_gbc_max_depth, predict=False):\n",
    "    # copy source data from static\n",
    "    train = train_static.copy(deep=True)\n",
    "    test = test_static.copy(deep=True)\n",
    "    \n",
    "    # Feature Engineering    \n",
    "    # 'Z' is free\n",
    "    train['Cabin'].fillna('Z', inplace=True)\n",
    "    test['Cabin'].fillna('Z', inplace=True)\n",
    "    \n",
    "    # embarked\n",
    "    train['Embarked'].fillna('A', inplace=True)\n",
    "    test['Embarked'].fillna('A', inplace=True)\n",
    "    \n",
    "    # Fare\n",
    "    train['Fare'].fillna(0, inplace=True)\n",
    "    test['Fare'].fillna(0, inplace=True)\n",
    "    \n",
    "    # create feature from cabin\n",
    "    def contained_letter(value):\n",
    "        for letter in string.ascii_lowercase:\n",
    "            if letter.capitalize() in value:\n",
    "                return letter.capitalize()\n",
    "        return 'Z'\n",
    "    train['Cabin'] = train['Cabin'].apply(lambda x: contained_letter(x))\n",
    "    test['Cabin'] = test['Cabin'].apply(lambda x: contained_letter(x))\n",
    "    \n",
    "    # prepare one-hot-encoded columns\n",
    "    df_train_dummy = pd.get_dummies(train.drop(['Survived','PassengerId'], axis=1))\n",
    "    df_test_dummy = pd.get_dummies(test.drop(['PassengerId'], axis=1))\n",
    "    \n",
    "    # handle missing columns\n",
    "    df_test_dummy = df_test_dummy.reindex(columns=df_train_dummy.columns, fill_value=0)\n",
    "    df_train_dummy = df_train_dummy.reindex(columns=df_test_dummy.columns, fill_value=0)\n",
    "    \n",
    "    # age - imputate\n",
    "    imputer = KNNImputer(n_neighbors=hp_knn_neighbors, weights=\"distance\") # distance might be better than \"uniform\"\n",
    "    imputer.fit(df_train_dummy) # In test set we wont know the \"survived\" feature and \"PassengerId\" is no use for imputation\n",
    "    # train\n",
    "    train_dummy = pd.DataFrame(imputer.transform(df_train_dummy), columns=df_train_dummy.columns)\n",
    "    # test\n",
    "    test_dummy = pd.DataFrame(imputer.transform(df_test_dummy), columns=df_test_dummy.columns)\n",
    "    \n",
    "    # Preprocessing\n",
    "    normalizer = Normalizer()\n",
    "    # fit train\n",
    "    train_dummy.loc[:,['Age','Fare']] = normalizer.fit_transform(train_dummy.loc[:,['Age','Fare']])\n",
    "    # transform test\n",
    "    test_dummy.loc[:,['Age','Fare']] = normalizer.transform(test_dummy.loc[:,['Age','Fare']])\n",
    "    \n",
    "    # Init GBC\n",
    "    clf = GradientBoostingClassifier(n_estimators=hp_gbc_estimators, learning_rate=hp_gbc_learning_rate, max_depth=hp_gbc_max_depth, random_state=0)\n",
    "    clf.fit(train_dummy, train['Survived'])\n",
    "    \n",
    "    # predict survival\n",
    "    if predict:\n",
    "        predictions = pd.DataFrame(clf.predict(test_dummy), columns=['Survived'])\n",
    "        predictions.loc[:,'PassengerId'] = test.loc[:,'PassengerId']\n",
    "        return predictions\n",
    "    else:\n",
    "        return clf.score(test_dummy, gender_submission['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f574c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "from skopt.optimizer import Optimizer\n",
    "import skopt.space as HP_dtypes\n",
    "\n",
    "knn_neighbors = HP_dtypes.Integer(low=2, high=10)\n",
    "gbc_estimators = HP_dtypes.Integer(low=2, high=100)\n",
    "gbc_learning_rate = HP_dtypes.Real(low=1e-5, high=1)\n",
    "gbc_max_depth = HP_dtypes.Integer(low=1, high=10)\n",
    "\n",
    "opt = Optimizer(dimensions=[knn_neighbors,\n",
    "                            gbc_estimators,\n",
    "                            gbc_learning_rate,\n",
    "                            gbc_max_depth],\n",
    "                acq_func=\"EI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "201d95eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #0 score: 0.7990430622009569\n",
      "Iteration #1 score: 0.8181818181818182\n",
      "Iteration #2 score: 0.8564593301435407\n",
      "Iteration #3 score: 0.7990430622009569\n",
      "Iteration #4 score: 0.8516746411483254\n",
      "Iteration #5 score: 0.8421052631578947\n",
      "Iteration #6 score: 0.7870813397129187\n",
      "Iteration #7 score: 0.8157894736842105\n",
      "Iteration #8 score: 0.9282296650717703\n",
      "Iteration #9 score: 0.9641148325358851\n",
      "Iteration #10 score: 0.6363636363636364\n",
      "Iteration #11 score: 0.8755980861244019\n",
      "Iteration #12 score: 0.8277511961722488\n",
      "Iteration #13 score: 0.8947368421052632\n",
      "Iteration #14 score: 0.9186602870813397\n",
      "Iteration #15 score: 0.937799043062201\n",
      "Iteration #16 score: 0.6363636363636364\n",
      "Iteration #17 score: 0.9880382775119617\n",
      "Iteration #18 score: 0.8205741626794258\n",
      "Iteration #19 score: 0.9880382775119617\n",
      "Iteration #20 score: 0.6363636363636364\n",
      "Iteration #21 score: 0.8995215311004785\n",
      "Iteration #22 score: 0.992822966507177\n",
      "Iteration #23 score: 0.9856459330143541\n",
      "Iteration #24 score: 0.8229665071770335\n",
      "Iteration #25 score: 0.9401913875598086\n",
      "Iteration #26 score: 0.9401913875598086\n",
      "Iteration #27 score: 0.8349282296650717\n",
      "Iteration #28 score: 0.9258373205741627\n",
      "Iteration #29 score: 0.8995215311004785\n",
      "Iteration #30 score: 0.6363636363636364\n",
      "Iteration #31 score: 0.9401913875598086\n",
      "Iteration #32 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# run loop\n",
    "for i in range(0,50):\n",
    "    # get hps\n",
    "    hps = opt.ask()\n",
    "    # run fnc\n",
    "    score = opt_fnc(*hps)\n",
    "    print(\"Iteration #{0} score: {1}\".format(i, score))\n",
    "    # update optimizer\n",
    "    opt.tell(x=hps, y=(1-score)) # (1-score) -> EI acq_func => trying to\n",
    "    if score == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b3b78de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 4, 0.5313535171711488, 1]\n"
     ]
    }
   ],
   "source": [
    "# retrieve best\n",
    "best_hps = opt.get_result().x\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e812cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "      <td>1305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  PassengerId\n",
       "0           0          892\n",
       "1           1          893\n",
       "2           0          894\n",
       "3           0          895\n",
       "4           1          896\n",
       "..        ...          ...\n",
       "413         0         1305\n",
       "414         1         1306\n",
       "415         0         1307\n",
       "416         0         1308\n",
       "417         0         1309\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the results\n",
    "df_predicted = opt_fnc(*hps, predict=True)\n",
    "df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef4c36dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       266\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       418\n",
      "   macro avg       1.00      1.00      1.00       418\n",
      "weighted avg       1.00      1.00      1.00       418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(gender_submission['Survived'], df_predicted['Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a34502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "df_predicted.to_csv(\"submission.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
